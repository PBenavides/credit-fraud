{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Answers\n",
    "\n",
    "### 1) Train a classifier to predict if a transaction is a fraudulent transaction or not?\n",
    "\n",
    "From Notebooks:\n",
    "\n",
    "**1.- Split dataset Notebook**\n",
    "\n",
    "- Since Time was measured with respect to the first transaction, it has to be transformed with a cyclical method to have the same measure in a production environment. Datasets were transformed first and then splited on train & test.\n",
    "\n",
    "\n",
    "- The data were split in a stratify manner to mantain the class proportions across train and test samples.\n",
    "\n",
    "\n",
    "**2.- Model Experimentation Notebook**\n",
    "\n",
    "This notebook was developed to experiment with many models using the Pycaret AutoML framework. Since the features' meanings are confidential, this doesnâ€™t allow us to directly add intuition to the data. The process involved three main data scenarios: Almost raw data scenario, fixing imbalance with Smote scenario and automated feature engineer scenario. Based on the previous approach the final model was determined.\n",
    "\n",
    "The main results for these Pycaret experiments for the three choosen models were: \n",
    "\n",
    "| Method/Model | ExtraTrees | XGBoost | LDA |\n",
    "| --- | --- | --- | --- |\n",
    "| Normalize | 86% | 85.64% | 81.55% |\n",
    "| SMOTE | 86% | 80% | 16% |\n",
    "| Auto Feat. Eng | 80.5% | 82% | 81% |\n",
    "\n",
    "<center>Taking F1-Score Metric</center>\n",
    "\n",
    "Latter, the experiments turned to be around **AUC-PR** metric. This was choosen due to the imbalanced target class and to have a better understanding on how the model is performing against precision-recall tradeoff.\n",
    "\n",
    "The final trained model was a VotingClassifier made out of XGBoost, LDA and Extratrees. Tuned models were developed by train dataset and then it were re-trained.\n",
    "\n",
    "*Note:  Functions were build in a re-usable way. These functions were also involved to try-error the models quickly but are not necessary used in the final notebook version. Nevertheless, these can be use for training environments.*\n",
    "\n",
    "**3.- Model Delivering Notebook**\n",
    "\n",
    "The main utility for this notebook is to create the models and artifacts needed for the API Pipeline. There are two relevant functions built here: ``build_model`` and ``tuning_job`` both being self-explainatory.\n",
    "\n",
    "Final Score against test dataset: \n",
    "\n",
    "| Method/Model | XGBoost| \n",
    "| --- | --- \n",
    "| AUC-PRC | 81.75% |\n",
    "| F1-score | 80 % | \n",
    "| Recall | 73.4% | \n",
    "| Precision | 90 % | \n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 2) How would you deploy this model in production?\n",
    "\n",
    "I would build the model as an API-REST endpoint using a backend framework (Flask, FastAPI, Django, etc) and Docker. Docker will be used to deploy and expose the service in hands with a cloud-engine helpful features. Starting with having a MVP model and building the entire pipeline, I'd continue interating over the model versions to fullfill client requirements.\n",
    "\n",
    "#### a) Create an API to deploy this model and use it in a dev environment\n",
    "\n",
    "API repo environment here: [GitHub repo link](https://github.com/PBenavides/credit-fraud/blob/main/dev/app/utils.py)\n",
    "\n",
    "#### b) API response and should be clear and should handle missing data and expections.\n",
    "\n",
    "You can see API responses and small handle errors in this project's script. [routes.py](https://github.com/PBenavides/credit-fraud/blob/main/dev/app/routes.py) and [utils.py](https://github.com/PBenavides/credit-fraud/blob/main/dev/app/utils.py)\n",
    "\n",
    "#### c) You are expected to share how your model could be called. (Postman, Request script)\n",
    "\n",
    "It can be called on both ways after running the app. There is script for small testing errors in [tests.py](https://github.com/PBenavides/credit-fraud/tree/main/dev)\n",
    "\n",
    "````Python\n",
    "#Request Script\n",
    "import requests\n",
    "\n",
    "sample_data = {\"V1\": -0.365234375, \"V2\": 0.1234415820, ... \"V28\": -0.00994110107421875,\"Amount\": 88.67}  #Input Features without 'Time'\n",
    "\n",
    "api_endpoint = 'http://127.0.0.1:5000/predict' #If running in local \n",
    "\n",
    "response = requests.post(api_endpoint, json=sample_data) # POST REQUEST\n",
    "\n",
    "print(response) #Should be [200] if everything's okay. data is in response.content\n",
    "\n",
    "````\n",
    "\n",
    "#### d) How could you deploy this model for a production setting? A paragraph only please.\n",
    "\n",
    "I would build the production environment apart from the training environment, using a framework that allows me to expose the model as an API and having a version control system of it so I can update it if necessary. Then, I would configure everything in a Docker container to be able to start the application on-demand and depending on how many requests I receive distribute the load. This would be orchestrated by some cloud service and using Kubernetes as a container manager. Finally, it would have a tracking service for production performance monitoring.\n",
    "\n",
    "\n",
    "#### e) How do you scale your application ? A paragraph only please\n",
    "\n",
    "Having a containarized API, I could use a cloud service to be able to serve container on demand load. It is also important to have a control of the consumption and memory metrics that the application may have, in order to have an idea of which service can be used and when it will be required to scale.\n",
    "\n",
    "\n",
    "#### f) How do you do performance testing for your API ? A paragraph only please\n",
    "\n",
    "Testing can be done from two approaches: the API and the model. For the former, I would generate automatic and stress test to the API by making several requests to find cases where the system fails, and track the response metrics. On the model side, I would track input and output distribution of the data to see if they change and if it needs to be retrained. If so, I would make first and A/B test and then change the production model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pablo Benavides**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
