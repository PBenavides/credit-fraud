{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center> Model Delivering </center>\n",
    "\n",
    "This notebook it's about train, tune and deliver the final models. We will save some artifacts to preprocess features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN SHAPE: (256326, 33)\n",
      "TEST SHAPE:(28481, 33)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import gc\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold, RepeatedStratifiedKFold, cross_val_score\n",
    "\n",
    "#SELECTED MODELS\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "#LOAD DATA.\n",
    "data_path=(\n",
    "    '../input/',\n",
    "    )\n",
    "\n",
    "#ARTIFACTS PATH\n",
    "artifacts_path = (\n",
    "    '../artifacts/models/',\n",
    "    '../artifacts/'\n",
    "    )\n",
    "\n",
    "train = pd.read_csv(data_path[0]+'train.csv')\n",
    "test = pd.read_csv(data_path[0]+'test.csv')\n",
    "\n",
    "print('TRAIN SHAPE: {}\\nTEST SHAPE:{}'.format(train.shape, test.shape))\n",
    "\n",
    "#DROP TIME feature\n",
    "train.drop('Time',axis=1, inplace=True)\n",
    "test.drop('Time',axis=1,inplace=True)\n",
    "\n",
    "#DEFINE Features and target\n",
    "features = train.drop('Class', axis=1).columns.to_list()\n",
    "target = 'Class'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reducing data Memory:**\n",
    "\n",
    "*Source: https://gist.github.com/fujiyuu75/748bc168c9ca8a49f86e144a08849893*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 62.58 MB\n",
      "Memory usage after optimization is: 15.40 MB\n",
      "Decreased by 75.4%\n",
      "Memory usage of dataframe is 6.95 MB\n",
      "Memory usage after optimization is: 1.71 MB\n",
      "Decreased by 75.4%\n"
     ]
    }
   ],
   "source": [
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        #else:\n",
    "        #    df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df\n",
    "\n",
    "train = reduce_mem_usage(train)\n",
    "test = reduce_mem_usage(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "The preprocessing steps will be:\n",
    "\n",
    "- Normalize data: artifact needed is mean and std for each data column.\n",
    "\n",
    "- Create extra features: based on the previous automate feat eng."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "artifacts_path = (\n",
    "    '../artifacts/models/',\n",
    "    '../artifacts/'\n",
    "    )\n",
    "\n",
    "x_train = train[features]\n",
    "x_test = test[features]\n",
    "\n",
    "all_data = pd.concat([x_train, x_test],axis=0) #Train the Normalizer with all the data\n",
    "\n",
    "normalizer = Normalizer().fit(all_data) # Fit inot the model\n",
    "\n",
    "norm_train = normalizer.transform(x_train.values)\n",
    "x_train = pd.DataFrame(norm_train, index=x_train.index, columns=x_train.columns)\n",
    "\n",
    "pickle.dump(normalizer, open(artifacts_path[1]+'normalizer.sav', 'wb')) #Save Normalizer as artifact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and Tune Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, auc, f1_score, recall_score, precision_score\n",
    "\n",
    "def auc_precision_recall_curve(y_true, y_preds):\n",
    "    \"\"\"Kaggle official doc from the data recommends this metric\n",
    "    \"\"\"\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_preds)\n",
    "    #AUC function to calculate AUC of precision recall curve\n",
    "    auc_precision_recall = auc(recall, precision)\n",
    "    return auc_precision_recall\n",
    "\n",
    "def compute_scores(y_true,y_preds):\n",
    "    \"\"\"Return a dictionary of results.\n",
    "    \n",
    "    It computes 4 metrics for Fraud Detection interests.\n",
    "    Arguments:\n",
    "    \n",
    "    y_true: real labeled data\n",
    "    y_preds: prediction from the model\n",
    "    \"\"\"\n",
    "    \n",
    "    scores = {\n",
    "        'AUC-PRC': auc_precision_recall_curve(y_true, y_preds),\n",
    "        'F1-score': f1_score(y_true, y_preds),\n",
    "        'Recall': recall_score(y_true, y_preds),\n",
    "        'Precision': precision_score(y_true, y_preds)\n",
    "    }\n",
    "    \n",
    "    return scores\n",
    "\n",
    "def score_report(score_dict, train=False):\n",
    "    \"\"\"Printed scores report\n",
    "    score_dict: dict from compute_scores function output.\n",
    "    \"\"\"\n",
    "    if train:\n",
    "        print('---'*10)\n",
    "        print('Train Scores:\\n')\n",
    "        for score_name, score_value in score_dict.items():\n",
    "            print(f\"{score_name}:  {score_value}\")\n",
    "    else:\n",
    "        print('Test Scores:\\n')\n",
    "        for score_name, score_value in score_dict.items():\n",
    "            print(f\"{score_name}:  {score_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(model, train_data, test_data, features, target, name, \n",
    "                artifacts_path='../artifacts/models/', tune=False, param_grid=None, final=True):\n",
    "    \"\"\" Train a Sklearn format model and make the final test\n",
    "    --------------\n",
    "    Parameters:\n",
    "    model: Model Instance.\n",
    "    train_dataset: training pd.DataFrame dataset\n",
    "    test_dataset: HoldOut pd.DataFrame dataset\n",
    "    features: List of features to be included.\n",
    "    target: target name\n",
    "    name: name to store the model\n",
    "    tune: If tuning job is calling.\n",
    "    returns a model instance\n",
    "    \"\"\"\n",
    "    \n",
    "    #  for testing against test_data(not seeing until this point)\n",
    "    \n",
    "    x_train, y_train = train_data[features], train_data[target]\n",
    "    x_test, y_test = test_data[features], test_data[target]\n",
    "    \n",
    "    if tune == True:  #Tuning job if necessary\n",
    "        \n",
    "        scorer = make_scorer(auc_precision_recall_curve, \n",
    "                             greater_is_better=True) #To optimize over AUC-RC\n",
    "        \n",
    "        model, results = tuning_job(\n",
    "                            model=model,\n",
    "                            data=train_data,\n",
    "                            features=features,\n",
    "                            target=target,\n",
    "                            param_grid=param_grid,\n",
    "                            cv=cv,\n",
    "                            scoring=scorer\n",
    "        )\n",
    "        \n",
    "    elif tune == False:\n",
    "        \n",
    "        model.fit(x_train, y_train)  #When final_model this is alos required for testing metrics.\n",
    "    \n",
    "    y_test_pred = model.predict(np.float32(x_test))  #xgboost doesn't allow dataframe, it must be floats\n",
    "    \n",
    "    #SCORES\n",
    "    test_results = compute_scores(y_test, y_test_pred)\n",
    "    score_report(test_results)\n",
    "    \n",
    "    #STORE\n",
    "    filename = artifacts_path + name +'.sav'\n",
    "    pickle.dump(model, open(filename, 'wb'))\n",
    "    \n",
    "    # INFERENCE TIME\n",
    "    random_sample = x_test.sample(n=1)\n",
    "    \n",
    "    start = time.time()\n",
    "    one_inf = model.predict(np.float32(random_sample))\n",
    "    end = time.time()\n",
    "    \n",
    "    print('One inference time:', end - start)\n",
    "    \n",
    "    #Re-train with all data if final==True\n",
    "    if final==True: #if model is final\n",
    "        all_data = pd.concat([train_data, test_data], axis=0)\n",
    "        model.fit(np.float32(all_data[features]), np.float32(all_data[target]))\n",
    "        return model\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pablo\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:143: FutureWarning: The sklearn.metrics.scorer module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics.scorer import make_scorer\n",
    "\n",
    "def tuning_job(model, data, features, target, param_grid,scorer, cv=5):\n",
    "    \"\"\"Tunes the model and outputs best fit.\n",
    "    ----------\n",
    "    Parameters:\n",
    "    model: Sklearn instance or Sklearn wrapper\n",
    "    data: x_train data.\n",
    "    param_grid: search space of hyperparameters\n",
    "    \"\"\"\n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "                    estimator = model,\n",
    "                    param_grid=param_grid,\n",
    "                    cv=cv,\n",
    "                    scoring=scorer\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(np.float32(data[features]), np.float32(data[target]))\n",
    "    \n",
    "    results = grid_search.cv_results_\n",
    "    \n",
    "    best_estimator = grid_search.best_estimator_\n",
    "    \n",
    "    return best_estimator, results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Delivering:\n",
    "\n",
    "**1.- LDA**\n",
    "\n",
    "LDA does not need to be tuned. \n",
    "\n",
    "*Source: https://datascience.stackexchange.com/questions/21942/linear-discriminant-analysis-which-parameters-can-be-tunned-in-cross-validation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Scores:\n",
      "\n",
      "AUC-PRC:  0.7077814377743654\n",
      "F1-score:  0.7032967032967032\n",
      "Recall:  0.6530612244897959\n",
      "Precision:  0.7619047619047619\n",
      "One inference time: 0.000997781753540039\n"
     ]
    }
   ],
   "source": [
    "#FOR LDA. \n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "model = LinearDiscriminantAnalysis()\n",
    "features = train.drop('Class', axis=1).columns.to_list()\n",
    "target = 'Class'\n",
    "name='first_lda'\n",
    "\n",
    "lda_model = build_model(model, train, test, features, target, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.- Extra Trees**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Scores:\n",
      "\n",
      "AUC-PRC:  0.8179149508193432\n",
      "F1-score:  0.8045977011494253\n",
      "Recall:  0.7142857142857143\n",
      "Precision:  0.9210526315789473\n",
      "One inference time: 0.022939443588256836\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "model = ExtraTreesClassifier(n_jobs=-1)\n",
    "name = 'first_et'\n",
    "et = build_model(model, train, test, features, target, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GrindSearch Results:\n",
      " {'mean_fit_time': array([26.84591489,  4.64906516,  3.36830096,  7.20164361,  3.9055057 ,\n",
      "        3.96377382, 11.15993519,  3.57548676,  3.15751328, 13.71353145,\n",
      "        3.14232178,  2.44076271]), 'std_fit_time': array([5.0747395 , 0.87329246, 0.73621437, 1.00676932, 1.47386738,\n",
      "       1.02449758, 0.23057305, 0.42737733, 0.40138549, 1.95225189,\n",
      "       1.10233683, 0.20201187]), 'mean_score_time': array([1.06514654, 0.30422077, 0.22383952, 0.27955313, 0.22403827,\n",
      "       0.23425202, 0.41798396, 0.27053308, 0.20831175, 0.4546319 ,\n",
      "       0.26361718, 0.18921418]), 'std_score_time': array([0.41180104, 0.07648922, 0.05909967, 0.08515743, 0.04513269,\n",
      "       0.03960724, 0.02948248, 0.03128417, 0.06054472, 0.1911039 ,\n",
      "       0.08305488, 0.02321461]), 'param_max_depth': masked_array(data=[None, None, None, 5, 5, 5, 10, 10, 10, 12, 12, 12],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_min_impurity_decrease': masked_array(data=[0, 0.2, 0.5, 0, 0.2, 0.5, 0, 0.2, 0.5, 0, 0.2, 0.5],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'max_depth': None, 'min_impurity_decrease': 0}, {'max_depth': None, 'min_impurity_decrease': 0.2}, {'max_depth': None, 'min_impurity_decrease': 0.5}, {'max_depth': 5, 'min_impurity_decrease': 0}, {'max_depth': 5, 'min_impurity_decrease': 0.2}, {'max_depth': 5, 'min_impurity_decrease': 0.5}, {'max_depth': 10, 'min_impurity_decrease': 0}, {'max_depth': 10, 'min_impurity_decrease': 0.2}, {'max_depth': 10, 'min_impurity_decrease': 0.5}, {'max_depth': 12, 'min_impurity_decrease': 0}, {'max_depth': 12, 'min_impurity_decrease': 0.2}, {'max_depth': 12, 'min_impurity_decrease': 0.5}], 'split0_test_score': array([0.84464287, 0.50086802, 0.50086802, 0.68925059, 0.50086802,\n",
      "       0.50086802, 0.77397718, 0.50086802, 0.50086802, 0.82123246,\n",
      "       0.50086802, 0.50086802]), 'split1_test_score': array([0.89495007, 0.50085829, 0.50085829, 0.68549167, 0.50085829,\n",
      "       0.50085829, 0.81608502, 0.50085829, 0.50085829, 0.85271762,\n",
      "       0.50085829, 0.50085829]), 'split2_test_score': array([0.8735337 , 0.50085829, 0.50085829, 0.61847317, 0.50085829,\n",
      "       0.50085829, 0.77266703, 0.50085829, 0.50085829, 0.82834763,\n",
      "       0.50085829, 0.50085829]), 'split3_test_score': array([0.85397005, 0.50086804, 0.50086804, 0.60138385, 0.50086804,\n",
      "       0.50086804, 0.8044603 , 0.50086804, 0.50086804, 0.84168675,\n",
      "       0.50086804, 0.50086804]), 'split4_test_score': array([0.86794158, 0.50086804, 0.50086804, 0.64107365, 0.50086804,\n",
      "       0.50086804, 0.78127925, 0.50086804, 0.50086804, 0.84637367,\n",
      "       0.50086804, 0.50086804]), 'mean_test_score': array([0.86700765, 0.50086413, 0.50086413, 0.64713458, 0.50086413,\n",
      "       0.50086413, 0.78969376, 0.50086413, 0.50086413, 0.83807163,\n",
      "       0.50086413, 0.50086413]), 'std_test_score': array([1.72881590e-02, 4.77533272e-06, 4.77533272e-06, 3.52033110e-02,\n",
      "       4.77533272e-06, 4.77533272e-06, 1.74486296e-02, 4.77533272e-06,\n",
      "       4.77533272e-06, 1.16156094e-02, 4.77533272e-06, 4.77533272e-06]), 'rank_test_score': array([1, 5, 5, 4, 5, 5, 3, 5, 5, 2, 5, 5])}\n"
     ]
    }
   ],
   "source": [
    "#Tune model: test ET\n",
    "param_grid = {'max_depth':[None, 5,10,12], 'min_impurity_decrease':[0,0.2,0.5]}\n",
    "\n",
    "scorer = make_scorer(auc_precision_recall_curve, \n",
    "                             greater_is_better=True)\n",
    "\n",
    "best_et, results = tuning_job(model, train, features, target, param_grid,scorer, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Scores:\n",
      "\n",
      "AUC-PRC:  0.8179149508193432\n",
      "F1-score:  0.8045977011494253\n",
      "Recall:  0.7142857142857143\n",
      "Precision:  0.9210526315789473\n",
      "One inference time: 0.08518028259277344\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ExtraTreesClassifier(min_impurity_decrease=0, n_jobs=-1)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name='tuned_et'\n",
    "build_model(best_et, train, test, features, target, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.- XGBOOST**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:22:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Test Scores:\n",
      "\n",
      "AUC-PRC:  0.8289811993635815\n",
      "F1-score:  0.8222222222222222\n",
      "Recall:  0.7551020408163265\n",
      "Precision:  0.9024390243902439\n",
      "One inference time: 0.0059850215911865234\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "model = XGBClassifier()\n",
    "name = 'first_xgb'\n",
    "xgb_ = build_model(model, train, test, features, target, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pablo\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:22:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:23:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:23:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:24:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:24:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:25:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:25:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:25:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:26:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:26:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:27:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:27:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:27:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:28:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:28:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:28:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:28:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:29:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:29:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "model = XGBClassifier(learning_rate=0.03)\n",
    "\n",
    "param_grid = {'eta':[0.1, 0.01, 0.4, 0.5], #Parameter tuning \n",
    "              'max_depth':[None, 5, 10, 15, 20,],\n",
    "              'alpha':[1,5,10,15,20],\n",
    "              'gamma':[1,3,5],\n",
    "              'lambda':[1,3,5,7]}\n",
    "\n",
    "xgb_tuned, results = tuning_job(model, train, features, target, param_grid,scorer, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'objective': 'binary:logistic',\n",
       " 'use_label_encoder': True,\n",
       " 'base_score': 0.5,\n",
       " 'booster': 'gbtree',\n",
       " 'colsample_bylevel': 1,\n",
       " 'colsample_bynode': 1,\n",
       " 'colsample_bytree': 1,\n",
       " 'gamma': 0,\n",
       " 'gpu_id': -1,\n",
       " 'importance_type': 'gain',\n",
       " 'interaction_constraints': '',\n",
       " 'learning_rate': 0.400000006,\n",
       " 'max_delta_step': 0,\n",
       " 'max_depth': 20,\n",
       " 'min_child_weight': 1,\n",
       " 'missing': nan,\n",
       " 'monotone_constraints': '()',\n",
       " 'n_estimators': 100,\n",
       " 'n_jobs': 4,\n",
       " 'num_parallel_tree': 1,\n",
       " 'random_state': 0,\n",
       " 'reg_alpha': 0,\n",
       " 'reg_lambda': 1,\n",
       " 'scale_pos_weight': 1,\n",
       " 'subsample': 1,\n",
       " 'tree_method': 'exact',\n",
       " 'validate_parameters': 1,\n",
       " 'verbosity': None,\n",
       " 'eta': 0.4}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_tuned.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:36:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Test Scores:\n",
      "\n",
      "AUC-PRC:  0.8175751610991646\n",
      "F1-score:  0.8089887640449438\n",
      "Recall:  0.7346938775510204\n",
      "Precision:  0.9\n",
      "One inference time: 0.015623331069946289\n"
     ]
    }
   ],
   "source": [
    "name = 'tuned_xgb'\n",
    "tuned_xgb = build_model(xgb_tuned, train, test, features, target, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking Models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "#DEFINE ESTIMATORS\n",
    "estimators = [\n",
    "    ('LDA', lda_model),\n",
    "    ('ET', best_et),\n",
    "    ('XGB', tuned_xgb)\n",
    "]\n",
    "\n",
    "name='voting_final'\n",
    "model_voting = VotingClassifier(estimators=estimators, voting='hard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pablo\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:25:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Test Scores:\n",
      "\n",
      "AUC-PRC:  0.806661568420034\n",
      "F1-score:  0.7906976744186046\n",
      "Recall:  0.6938775510204082\n",
      "Precision:  0.918918918918919\n",
      "One inference time: 0.042886972427368164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pablo\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:27:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "#BUILDING MODEL: features & targets are the same as previous\n",
    "final_voting = build_model(model_voting, train, test, features, target, name, final=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this, we encountered that Voting Classifier model was overfitted. We will change to XGBoost as a Final model to make inferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:17:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Test Scores:\n",
      "\n",
      "AUC-PRC:  0.8079942453079945\n",
      "F1-score:  0.8043478260869564\n",
      "Recall:  0.7551020408163265\n",
      "Precision:  0.8604651162790697\n",
      "One inference time: 0.0049855709075927734\n",
      "[18:19:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "tuned_xgb = XGBClassifier(max_depth = 20, learning_rate=0.03, eta=0.4)\n",
    "name = 'final_xgb'\n",
    "final_xgb = build_model(tuned_xgb, train, test, features, target, name, final=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
